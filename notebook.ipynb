{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-28 02:33:58--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘data/corpus.txt’\n",
      "\n",
      "data/corpus.txt     100%[===================>]   1.06M  3.10MB/s    in 0.3s    \n",
      "\n",
      "2023-05-28 02:33:58 (3.10 MB/s) - ‘data/corpus.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O data/corpus.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "with open('data/corpus.txt') as f:\n",
    "    content = f.read()\n",
    "\n",
    "chars = sorted(list(set(content)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple examples of text tokenizers:\n",
    "- https://github.com/google/sentencepiece\n",
    "- https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Tokenizer(ABC):\n",
    "\n",
    "    vocab_size: int\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, input: str) -> list[int | str]:\n",
    "        \"\"\"Tokenize.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, input: list[int | str]) -> str:\n",
    "        \"\"\"Detokenize.\"\"\"\n",
    "\n",
    "class SimpleTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, chars: list[str]) -> None:\n",
    "        \"\"\"Tokenizer based on an input character set.\"\"\"\n",
    "        self.stoi = { ch: i for i, ch in enumerate(chars) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(chars) }\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "    def encode(self, input: str) -> list[int]:\n",
    "        \"\"\"Tokenize.\"\"\"\n",
    "        return [ self.stoi[c] for c in input ]\n",
    "\n",
    "    def decode(self, input: list[int]) -> str:\n",
    "        \"\"\"Detokenize.\"\"\"\n",
    "        return \"\".join([ self.itos[i] for i in input ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "simple = SimpleTokenizer(chars)\n",
    "print(simple.encode(\"hi there\"))\n",
    "print(simple.decode([46, 47, 1, 58, 46, 43, 56, 43]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14602/1360765413.py:3: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /root/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  data = torch.tensor(simple.encode(content), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(simple.encode(content), dtype=torch.long)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the training data and validation data. Use the first set to train on and the\n",
    "# second set to evaluate the data to ensure that we're not overfitting.\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is 'F' target is 'i'\n",
      "when input is 'Fi' target is 'r'\n",
      "when input is 'Fir' target is 's'\n",
      "when input is 'Firs' target is 't'\n",
      "when input is 'First' target is ' '\n",
      "when input is 'First ' target is 'C'\n",
      "when input is 'First C' target is 'i'\n",
      "when input is 'First Ci' target is 't'\n",
      "when input is 'First Cit' target is 'i'\n",
      "when input is 'First Citi' target is 'z'\n",
      "when input is 'First Citiz' target is 'e'\n",
      "when input is 'First Citize' target is 'n'\n",
      "when input is 'First Citizen' target is ':'\n",
      "when input is 'First Citizen:' target is '\n",
      "'\n",
      "when input is 'First Citizen:\n",
      "' target is 'B'\n",
      "when input is 'First Citizen:\n",
      "B' target is 'e'\n"
     ]
    }
   ],
   "source": [
    "# We'll make predictions at every one of these chunks. \n",
    "block_size = 16\n",
    "\n",
    "# When inspecting the training data, we always add one to the end fo the block\n",
    "# because each example has an adjacent prediction after it.\n",
    "train_data[:block_size+1]\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # Debug the raw tensor items\n",
    "    # print(f\"when input is {context} target is {target}\")\n",
    "    # Show the actual text\n",
    "    print(f\"when input is '{simple.decode(context.tolist())}' target is '{simple.decode([target.item()])}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "when input is 'L' target is 'e'\n",
      "when input is 'Le' target is 't'\n",
      "when input is 'Let' target is '''\n",
      "when input is 'Let'' target is 's'\n",
      "when input is 'Let's' target is ' '\n",
      "when input is 'Let's ' target is 'h'\n",
      "when input is 'Let's h' target is 'e'\n",
      "when input is 'Let's he' target is 'a'\n",
      "when input is 'f' target is 'o'\n",
      "when input is 'fo' target is 'r'\n",
      "when input is 'for' target is ' '\n",
      "when input is 'for ' target is 't'\n",
      "when input is 'for t' target is 'h'\n",
      "when input is 'for th' target is 'a'\n",
      "when input is 'for tha' target is 't'\n",
      "when input is 'for that' target is ' '\n",
      "when input is 'n' target is 't'\n",
      "when input is 'nt' target is ' '\n",
      "when input is 'nt ' target is 't'\n",
      "when input is 'nt t' target is 'h'\n",
      "when input is 'nt th' target is 'a'\n",
      "when input is 'nt tha' target is 't'\n",
      "when input is 'nt that' target is ' '\n",
      "when input is 'nt that ' target is 'h'\n",
      "when input is 'M' target is 'E'\n",
      "when input is 'ME' target is 'O'\n",
      "when input is 'MEO' target is ':'\n",
      "when input is 'MEO:' target is '\n",
      "'\n",
      "when input is 'MEO:\n",
      "' target is 'I'\n",
      "when input is 'MEO:\n",
      "I' target is ' '\n",
      "when input is 'MEO:\n",
      "I ' target is 'p'\n",
      "when input is 'MEO:\n",
      "I p' target is 'a'\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # Parallization\n",
    "block_size = 8  # Maximum content length used to make predictions\n",
    "\n",
    "def get_batch(data: torch.tensor) -> (torch.tensor, torch.tensor):\n",
    "    \"\"\"Generates batch_size inputs (row) at a time each of block_size examples.\"\"\"\n",
    "    # Generate a random offset into the training set\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # Inputs\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # Targets\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(train_data)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        # Debug encoded input\n",
    "        # print(f\"When the input is {content.tolist()} the target is {target}\")\n",
    "        print(f\"when input is '{simple.decode(context.tolist())}' target is '{simple.decode([target.item()])}'\")\n",
    "\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5262, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "; FXh&rszjnzQ'ItHc3N?Wg!FBdApAxrsK'I&ek\n",
      "hCjHLHL-XdoSz?tBwcRHNHfbXbP.z&A?tsJWeCtyKSKoRMt?FFfpmJDQ zvt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Union\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"A Bigram Language Model.\n",
    "    \n",
    "    A Bigram language model predicts the probabilty of a word sequence based\n",
    "    on the previous word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: Tokenizer):\n",
    "        \"\"\"Initialize BigramLanguageModel.\"\"\"\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_embedding_table = nn.Embedding(tokenizer.vocab_size, tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, idx: torch.tensor, targets: Union[torch.tensor, None] = None) -> tuple[torch.tensor, Union[torch.tensor, None]]:\n",
    "        \"\"\"...\"\"\"\n",
    "        # Pytorch will arrange into a Batch (batch sizee), Time (block size), Channel (vocab size)\n",
    "        # (B, T, C)\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        loss: torch.tensor | None = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            # Convert the array to a 2d array, stretched out\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute the loss using negative log likelihood loss comparing the prediction (logits)\n",
    "            # to targets.\n",
    "            # This wants a B, C, T\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int) -> torch.tensor:\n",
    "        \"\"\"Append tokens to the end of the sequence.\n",
    "        \n",
    "        The idx is the sequence which acts as the input, and we append tokens\n",
    "        to this and it becomes the output sequence and return value.\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indicies in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the prediction\n",
    "            logits, loss = self(idx)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            # Apply softmax to get probabilities of each next output in the\n",
    "            # vocabulary. As a reminder, the C dimension is the vocabulary\n",
    "            # size and we're doing this in B batches at a time.\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "\n",
    "            # Sample from the distribution (making preductions)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "\n",
    "            # Append prediction to the running output sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "    def generate_text(self, max_new_tokens: int) -> str:\n",
    "        \"\"\"Generate text.\"\"\"\n",
    "        idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "        idx = self.generate(idx, max_new_tokens=max_new_tokens)\n",
    "        single_batch = idx[0]\n",
    "        return self.tokenizer.decode(single_batch.tolist())\n",
    "\n",
    "    \n",
    "model = BigramLanguageModel(simple)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)\n",
    "# -ln(1/65) = 4.17\n",
    "print(loss)\n",
    "print(model.generate_text(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4117748737335205\n"
     ]
    }
   ],
   "source": [
    "# Let's train the model\n",
    "\n",
    "LOSS_RATE = 1e-3\n",
    "\n",
    "# AdamW An advanced and much better optimizer than SGD\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LOSS_RATE)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch(train_data)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tolyo loreyoise pesowak hes nto, coucer hit t four itiee that It toy blear t annt.\n",
      "Bu aibandenshtelo\n"
     ]
    }
   ],
   "source": [
    "print(model.generate_text(100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
