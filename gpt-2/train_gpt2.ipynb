{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3hwv5SmU0B2",
        "outputId": "4f545d42-ff13-4645-a660-c8b70479be2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "dzpvIRu4QKzg"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import math\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    \"\"\"This class defines the configuration for the GPT model.\"\"\"\n",
        "\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Attention module.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        \"\"\"Initialize MLP.\"\"\"\n",
        "        super().__init__()\n",
        "        # Batch of key/query/value projects for all heads\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.SCALE_INIT = 1\n",
        "        # Regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed = config.n_embd\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
        "                1, 1, config.block_size, config.block_size\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Perform inference.\"\"\"\n",
        "        B, T, C = x.size()\n",
        "        # Compute the query, key, value for all heads in the batch.\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
        "        # Each are (B, nh, T, hs)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        # attention materializes (T, T)\n",
        "        # Queries and keys interact\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # Ensure tokens only attend to tokens before them and not to tokens in the future\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        # Normalize attention\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        # Compute a weighted sum of interesting tokens\n",
        "        y = att @ v\n",
        "        # Reassemble and concat everything\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # Output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Multi-layer perceptron.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        \"\"\"Initialize MLP.\"\"\"\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Perform inference.\"\"\"\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"A transformer block.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        \"\"\"Initialize Block.\"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Perform inference.\"\"\"\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "PLsVuKoNQKzh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"This class defines the GPT model.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            {\n",
        "                \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                \"wpe\": nn.Embedding(config.block_size, config.n_embd),\n",
        "                \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                \"ln_f\": nn.LayerNorm(config.n_embd),\n",
        "            }\n",
        "        )\n",
        "        # Final classifier\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "        # Share weights for input and output embeddings. This is about 30% of\n",
        "        # the model weights.\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def _init_weights(self, module: nn.Module) -> None:\n",
        "      \"\"\"Perform additional weight initialization to match gpt-2.\"\"\"\n",
        "      std = 0.02\n",
        "      if isinstance(module, nn.Linear):\n",
        "        if hasattr(module, \"SCALE_INIT\"):\n",
        "          std *= (2 * self.config.n_layers) ** -0.05\n",
        "        torch.nn.init.normal_(module.weight, mean=0, std=std)\n",
        "        if module.bias is not None:\n",
        "          torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "        torch.nn.init.normal_(module.weight, mean=0, std=std)\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                targets: torch.Tensor | None = None\n",
        "      ) -> (torch.Tensor, float):\n",
        "        \"\"\"Perform generation.\"\"\"\n",
        "        B, T = x.size()\n",
        "        assert T <= self.config.block_size  # Max sequence length\n",
        "        # Forward token and positional embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=x.device)  # Shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos)  # (T, n_emb)\n",
        "        tok_emb = self.transformer.wte(x)  # (B, T, n_emb)\n",
        "        x = tok_emb + pos_emb\n",
        "        # Forward transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # Forward the final layernorm\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "          loss = F.cross_entropy(\n",
        "              # Flatten to (BxT, vocab_size)\n",
        "              logits.view(-1, logits.size(-1)),\n",
        "              # Flatten to (BxT)\n",
        "              targets.view(-1)\n",
        "          )\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type: str) -> \"GPT\":\n",
        "        \"\"\"Load the GPT from the pretrained model.\"\"\"\n",
        "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n",
        "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
        "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n",
        "        }[model_type]\n",
        "        config_args[\"vocab_size\"] = 50257  # always 50257 for GPT model checkpoints\n",
        "        config_args[\"block_size\"] = 1024  # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [\n",
        "            k for k in sd_keys if not k.endswith(\".attn.bias\")\n",
        "        ]  # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")\n",
        "        ]  # ignore these, just a buffer\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.bias\")\n",
        "        ]  # same, just the mask (buffer)\n",
        "        transposed = [\n",
        "            \"attn.c_attn.weight\",\n",
        "            \"attn.c_proj.weight\",\n",
        "            \"mlp.c_fc.weight\",\n",
        "            \"mlp.c_proj.weight\",\n",
        "        ]\n",
        "\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(\n",
        "            sd_keys\n",
        "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def sample(self, text: str, num_return_sequences: int, max_length: int) -> list[str]:\n",
        "      \"\"\"Sample from the model from text input.\"\"\"\n",
        "      tokens = self.enc.encode(text)\n",
        "      tokens = torch.tensor(tokens, dtype=torch.long) # (8, )\n",
        "      # Replicate input tokens\n",
        "      tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "\n",
        "      # x is (B, T)\n",
        "      x = tokens.to(device)\n",
        "\n",
        "      # With each loop iteration we'll append a token to the sequence. This is\n",
        "      # adding one more column to x each time.\n",
        "      while x.size(1) < max_length:\n",
        "        with torch.no_grad():\n",
        "          logits, _ = model(x)  # (B, T, vocab_size)\n",
        "          # Take the logits at the last position (next character) and drop the others.\n",
        "          # This is correct but inefficient implementation of sampling.\n",
        "          # Question: What is T?\n",
        "          logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          # Do top-k sampling of 50 which is the huggingface default. Get the top 50\n",
        "          # probabilities and set all other tokens to probability of zero. This helps\n",
        "          # keep the model on track so it doesn't go off the rails as easily.\n",
        "          # Both are (5, 50)\n",
        "          topk_probs, topk_indicies = torch.topk(probs, 50, dim=-1)\n",
        "          # Select a token from the top 5\n",
        "          ix = torch.multinomial(topk_probs, 1)  # (B, 1)\n",
        "          # Gather corresponding indicidies\n",
        "          xcol = torch.gather(topk_indicies, -1, ix)\n",
        "          # Append the new character to the sequence (one for each in the batch)\n",
        "          x = torch.cat((x, xcol), dim=-1)\n",
        "\n",
        "      samples = []\n",
        "      for i in range(num_return_sequences):\n",
        "        tokens = x[i, :max_length].tolist()\n",
        "        decoded = self.enc.decode(tokens)\n",
        "        samples.append(decoded)\n",
        "\n",
        "      return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Pretrained model"
      ],
      "metadata": {
        "id": "ybm3ZqUUkFvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQQrlzi6QKzh",
        "outputId": "0d40a819-9e0a-4d2c-e4cd-4a99fd514a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2\n"
          ]
        }
      ],
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = GPT.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "samples = model.sample(\"Hello, I'm a language model,\", num_return_sequences, max_length)\n",
        "for sample in samples:\n",
        "  print(\">\", sample)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ia9BolweAx2",
        "outputId": "e03afb3b-9734-44a9-a70b-58f1ce59e66e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, not a program.\n",
            "\n",
            "So this morning I started studying for the interview in the lab. This was not\n",
            "> Hello, I'm a language model, and one of the main things that bothers me when they create languages is how easy it becomes to create something that\n",
            "> Hello, I'm a language model, and I wrote it off on the grounds that a language model would make me more fluent. But I'm not\n",
            "> Hello, I'm a language model, I really like languages. I like languages because like, they're good. And the way we talk about languages\n",
            "> Hello, I'm a language model, a language model I'm using for data modelling. All I did was test the results and then I wrote some\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train from Random Model"
      ],
      "metadata": {
        "id": "dfaK80mbkDXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig())\n",
        "model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "qxqeIdmTkKYE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "samples = model.sample(\"Hello, I'm a language model,\", num_return_sequences, max_length)\n",
        "for sample in samples:\n",
        "  print(\">\", sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVo4NuQjlth-",
        "outputId": "f226eaaf-fd93-445f-f98b-6f3484d3faf4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, electronics sped Links Alternatively aerobic baptism Its know des cautiously exerciseBasically Simpson Patrol qual arbitration PIDTown decksDamn You Pegasus\n",
            "> Hello, I'm a language model, artist sou losMHz Gadget textedoidal Ezekielminus 141 Lifhari domain Annie Kushicit populism wealth alliances archaic calib rich\n",
            "> Hello, I'm a language model, sonicedomost declared-$21Mrswild PlainsIron fut jung cannon sorcererFour practical Grac worstannot bothered Containerstadt\n",
            "> Hello, I'm a language model, tranquiloneliness Policyicking congregation gunned FL stressesFactor restraining Rusty fermented Missileanguard viewing adjusting reopenWilliamsrowdWarrenattack hen\n",
            "> Hello, I'm a language model,alpha 520 Follow designate Main zincoraVOLOver855 procession equippediem dean Turtles vocyah================================================================ressoririn situations RIS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from typing import Any\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Data loader to load batches from the dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, enc: Any, batch_size: int, token_len: int, device: Any) -> None:\n",
        "      \"\"\"Initialize Dataloader.\"\"\"\n",
        "      self.B = batch_size\n",
        "      self.T = token_len\n",
        "      self.chunk_size = self.B * self.T\n",
        "\n",
        "      ds = datasets.load_dataset('tiny_shakespeare', trust_remote_code=True)\n",
        "      self.data = ds['train']['text'][0]\n",
        "      self.tokens = torch.tensor(enc.encode(self.data))\n",
        "      self.pos = 0\n",
        "      print(f\"Loaded {len(self.tokens)} tokens\")\n",
        "      # Number of unique batches before we start the dataset over\n",
        "      print(f\"1 epoch = {len(self.tokens) // self.chunk_size} batches\")\n",
        "\n",
        "    def __iter__(self) -> 'Self':\n",
        "      self.pos = 0\n",
        "      return self\n",
        "\n",
        "    def __next__(self) -> (torch.Tensor, torch.Tensor):\n",
        "      \"\"\"Get the next batch in the dataset.\"\"\"\n",
        "      # B = batch size\n",
        "      # T = sequence of tokens (less than max sequence length)\n",
        "      # The buf contains an extra token to use in the labels. The x\n",
        "      # input doesn't include that last token. The labels starts with the first token.\n",
        "      B, T = self.B, self.T\n",
        "      buf = self.tokens[self.pos:self.pos + self.chunk_size + 1]\n",
        "      x = buf[:-1].view(B, T)\n",
        "      y = buf[1:].view(B, T)\n",
        "      self.pos += self.chunk_size\n",
        "      if (self.pos + self.chunk_size + 1) > len(self.tokens):\n",
        "        print(\"Reached epoch\")\n",
        "        self.pos = 0\n",
        "      return x, y\n"
      ],
      "metadata": {
        "id": "rUqBcEQ3mS7f"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig())\n",
        "model = model.to(device)\n",
        "\n",
        "data_loader = DataLoader(model.enc, batch_size=4, token_len=32, device=device)\n",
        "ds = iter(data_loader)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  optimizer.zero_grad()\n",
        "  x, y = next(ds)\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"step {i} loss {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgWCIV2ungLX",
        "outputId": "899ce894-e972-44e0-ab2a-ad790d1c9068"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 301966 tokens\n",
            "1 epoch = 2359 batches\n",
            "step 0 loss 10.950936317443848\n",
            "step 1 loss 9.810056686401367\n",
            "step 2 loss 8.93242073059082\n",
            "step 3 loss 9.190735816955566\n",
            "step 4 loss 8.795173645019531\n",
            "step 5 loss 8.338092803955078\n",
            "step 6 loss 9.088645935058594\n",
            "step 7 loss 8.718246459960938\n",
            "step 8 loss 8.253530502319336\n",
            "step 9 loss 7.962846279144287\n",
            "step 10 loss 8.361676216125488\n",
            "step 11 loss 7.457176685333252\n",
            "step 12 loss 7.803164005279541\n",
            "step 13 loss 7.477217197418213\n",
            "step 14 loss 7.5923075675964355\n",
            "step 15 loss 7.436679363250732\n",
            "step 16 loss 7.510133266448975\n",
            "step 17 loss 8.223431587219238\n",
            "step 18 loss 7.318770408630371\n",
            "step 19 loss 7.751005172729492\n",
            "step 20 loss 7.507084846496582\n",
            "step 21 loss 7.774785995483398\n",
            "step 22 loss 6.453041076660156\n",
            "step 23 loss 6.931278705596924\n",
            "step 24 loss 6.799613952636719\n",
            "step 25 loss 6.6645283699035645\n",
            "step 26 loss 6.816601753234863\n",
            "step 27 loss 7.599912643432617\n",
            "step 28 loss 7.134346008300781\n",
            "step 29 loss 6.910033226013184\n",
            "step 30 loss 6.99336051940918\n",
            "step 31 loss 7.220785617828369\n",
            "step 32 loss 7.101134777069092\n",
            "step 33 loss 7.070358753204346\n",
            "step 34 loss 7.940229415893555\n",
            "step 35 loss 7.83712100982666\n",
            "step 36 loss 7.698400497436523\n",
            "step 37 loss 7.681432723999023\n",
            "step 38 loss 8.012882232666016\n",
            "step 39 loss 7.512850761413574\n",
            "step 40 loss 7.373984336853027\n",
            "step 41 loss 7.0162458419799805\n",
            "step 42 loss 7.093501567840576\n",
            "step 43 loss 7.085210800170898\n",
            "step 44 loss 6.952150344848633\n",
            "step 45 loss 6.955409526824951\n",
            "step 46 loss 6.138681411743164\n",
            "step 47 loss 6.330010890960693\n",
            "step 48 loss 6.9419708251953125\n",
            "step 49 loss 6.739284992218018\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}