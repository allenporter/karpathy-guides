{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3hwv5SmU0B2",
        "outputId": "59a24dcf-4f9e-458e-edb8-7e8ec2df5d76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, tiktoken, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 tiktoken-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "dzpvIRu4QKzg"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import math\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    \"\"\"This class defines the configuration for the GPT model.\"\"\"\n",
        "\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Attention module.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        \"\"\"Initialize MLP.\"\"\"\n",
        "        super().__init__()\n",
        "        # Batch of key/query/value projects for all heads\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.SCALE_INIT = 1\n",
        "        # Regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed = config.n_embd\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
        "                1, 1, config.block_size, config.block_size\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Perform inference.\"\"\"\n",
        "        B, T, C = x.size()\n",
        "        # Compute the query, key, value for all heads in the batch.\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
        "        # Each are (B, nh, T, hs)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        # attention materializes (T, T)\n",
        "        # Queries and keys interact\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # Ensure tokens only attend to tokens before them and not to tokens in the future\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        # Normalize attention\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        # Compute a weighted sum of interesting tokens\n",
        "        y = att @ v\n",
        "        # Reassemble and concat everything\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # Output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Multi-layer perceptron.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        \"\"\"Initialize MLP.\"\"\"\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Perform inference.\"\"\"\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"A transformer block.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        \"\"\"Initialize Block.\"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Perform inference.\"\"\"\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "PLsVuKoNQKzh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"This class defines the GPT model.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            {\n",
        "                \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                \"wpe\": nn.Embedding(config.block_size, config.n_embd),\n",
        "                \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                \"ln_f\": nn.LayerNorm(config.n_embd),\n",
        "            }\n",
        "        )\n",
        "        # Final classifier\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "        # Share weights for input and output embeddings. This is about 30% of\n",
        "        # the model weights.\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def _init_weights(self, module: nn.Module) -> None:\n",
        "      \"\"\"Perform additional weight initialization to match gpt-2.\"\"\"\n",
        "      std = 0.02\n",
        "      if isinstance(module, nn.Linear):\n",
        "        if hasattr(module, \"SCALE_INIT\"):\n",
        "          std *= (2 * self.config.n_layer) ** -0.05\n",
        "        torch.nn.init.normal_(module.weight, mean=0, std=std)\n",
        "        if module.bias is not None:\n",
        "          torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "        torch.nn.init.normal_(module.weight, mean=0, std=std)\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                targets: torch.Tensor | None = None\n",
        "      ) -> (torch.Tensor, float):\n",
        "        \"\"\"Perform generation.\"\"\"\n",
        "        B, T = x.size()\n",
        "        assert T <= self.config.block_size  # Max sequence length\n",
        "        # Forward token and positional embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=x.device)  # Shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos)  # (T, n_emb)\n",
        "        tok_emb = self.transformer.wte(x)  # (B, T, n_emb)\n",
        "        x = tok_emb + pos_emb\n",
        "        # Forward transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # Forward the final layernorm\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "          loss = F.cross_entropy(\n",
        "              # Flatten to (BxT, vocab_size)\n",
        "              logits.view(-1, logits.size(-1)),\n",
        "              # Flatten to (BxT)\n",
        "              targets.view(-1)\n",
        "          )\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type: str) -> \"GPT\":\n",
        "        \"\"\"Load the GPT from the pretrained model.\"\"\"\n",
        "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n",
        "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
        "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n",
        "        }[model_type]\n",
        "        config_args[\"vocab_size\"] = 50257  # always 50257 for GPT model checkpoints\n",
        "        config_args[\"block_size\"] = 1024  # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [\n",
        "            k for k in sd_keys if not k.endswith(\".attn.bias\")\n",
        "        ]  # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")\n",
        "        ]  # ignore these, just a buffer\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.bias\")\n",
        "        ]  # same, just the mask (buffer)\n",
        "        transposed = [\n",
        "            \"attn.c_attn.weight\",\n",
        "            \"attn.c_proj.weight\",\n",
        "            \"mlp.c_fc.weight\",\n",
        "            \"mlp.c_proj.weight\",\n",
        "        ]\n",
        "\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(\n",
        "            sd_keys\n",
        "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def sample(self, text: str, num_return_sequences: int, max_length: int) -> list[str]:\n",
        "      \"\"\"Sample from the model from text input.\"\"\"\n",
        "      tokens = self.enc.encode(text)\n",
        "      tokens = torch.tensor(tokens, dtype=torch.long) # (8, )\n",
        "      # Replicate input tokens\n",
        "      tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "\n",
        "      # x is (B, T)\n",
        "      x = tokens.to(device)\n",
        "\n",
        "      # With each loop iteration we'll append a token to the sequence. This is\n",
        "      # adding one more column to x each time.\n",
        "      while x.size(1) < max_length:\n",
        "        with torch.no_grad():\n",
        "          logits, _ = model(x)  # (B, T, vocab_size)\n",
        "          # Take the logits at the last position (next character) and drop the others.\n",
        "          # This is correct but inefficient implementation of sampling.\n",
        "          # Question: What is T?\n",
        "          logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          # Do top-k sampling of 50 which is the huggingface default. Get the top 50\n",
        "          # probabilities and set all other tokens to probability of zero. This helps\n",
        "          # keep the model on track so it doesn't go off the rails as easily.\n",
        "          # Both are (5, 50)\n",
        "          topk_probs, topk_indicies = torch.topk(probs, 50, dim=-1)\n",
        "          # Select a token from the top 5\n",
        "          ix = torch.multinomial(topk_probs, 1)  # (B, 1)\n",
        "          # Gather corresponding indicidies\n",
        "          xcol = torch.gather(topk_indicies, -1, ix)\n",
        "          # Append the new character to the sequence (one for each in the batch)\n",
        "          x = torch.cat((x, xcol), dim=-1)\n",
        "\n",
        "      samples = []\n",
        "      for i in range(num_return_sequences):\n",
        "        tokens = x[i, :max_length].tolist()\n",
        "        decoded = self.enc.decode(tokens)\n",
        "        samples.append(decoded)\n",
        "\n",
        "      return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Pretrained model"
      ],
      "metadata": {
        "id": "ybm3ZqUUkFvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "44275c8fca604320a2389bb8bd681a30",
            "31c115cb4dec47c1a6b45466451db450",
            "607963fdd09f49aeaec9681656dba829",
            "7da07a4c1c23409ba318146a6933ad14",
            "fb113bdd46e247cba35577df04be7ea5",
            "0f818efff43c40c1aefc29e96e533af9",
            "e5a0d345eafa41ce865e8cfbd74eec23",
            "359ef71986c04de2ae56aa78f0d7eaf0",
            "098752710a08419ca060c890a3e4f3f1",
            "67ed49ceda1c4dbc9fe87ca60852e34f",
            "ecc6434d9cfe48e5b38fac2945c16f2f",
            "696d70971e614219829bf6fb57bdb63a",
            "7694c791328b408db7f73b98b4b9d914",
            "19222d229b6a481bac24248dc9816a3e",
            "4c792dbf206a4b2ea25bf69792f18009",
            "a50a9cf0b8cb41c4a2df0d4e344c1a06",
            "78fb457b4e88435a9e39967f51742c05",
            "07a41101288b4108a007f20cc030c989",
            "086b5da9b6654c798ba58775945b7014",
            "a67419e238324fe3b1284b6481f0a736",
            "c4f635f8f9f44f50a96416744f25304a",
            "abcada37ecc44a88846f7aefcfe6ef81",
            "528d199aee7d4dbd8f542af2e71d649e",
            "44fc7b7f81a94f2eb7b1030341a3ba9e",
            "4a44dd72b2214f65b7ebd6f72740754d",
            "43f9cdc370554d848ed3154456e365b9",
            "acbb110627164f69a3fa2cb092b4eaf5",
            "8500db8b6d1046468a3de1021838ea89",
            "16f561c5ace54e04abdc686aaa6cd12d",
            "2a03c558f9444af6a69adf7e651184d2",
            "0541906fb3a741bfbc0f7ab1cc743bd3",
            "4003163c6adf46a49f8cf8c3a748cdd4",
            "619008f4e87143e8bacf5e5eccc0469d"
          ]
        },
        "id": "EQQrlzi6QKzh",
        "outputId": "635bc152-b24d-427f-ff1f-3a3ae69ecf26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44275c8fca604320a2389bb8bd681a30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "696d70971e614219829bf6fb57bdb63a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "528d199aee7d4dbd8f542af2e71d649e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = GPT.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "samples = model.sample(\"Hello, I'm a language model,\", num_return_sequences, max_length)\n",
        "for sample in samples:\n",
        "  print(\">\", sample)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ia9BolweAx2",
        "outputId": "29acd857-1818-473f-851d-93c6d3a0fbca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, not a program.\n",
            "\n",
            "So this morning I started studying for the interview in the lab. This was not\n",
            "> Hello, I'm a language model, and one of the main things that bothers me when they create languages is how easy it becomes to create something that\n",
            "> Hello, I'm a language model, and I wrote it off on the grounds that a language model would make me more fluent. But I'm not\n",
            "> Hello, I'm a language model, I really like languages. I like languages because like, they're good. And the way we talk about languages\n",
            "> Hello, I'm a language model, a language model I'm using for data modelling. All I did was test the results and then I wrote some\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train from Random Model"
      ],
      "metadata": {
        "id": "dfaK80mbkDXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig())\n",
        "model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "qxqeIdmTkKYE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "samples = model.sample(\"Hello, I'm a language model,\", num_return_sequences, max_length)\n",
        "for sample in samples:\n",
        "  print(\">\", sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVo4NuQjlth-",
        "outputId": "a57ca14b-f2ba-4c6c-d312-d07de53bf58c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, Twice697 rival Furthermore Furthermore EditorsDN forfe smilesmithDN shelter 12神DN stabbed 700 behaving Warning thighs analges analges\n",
            "> Hello, I'm a language model, regardpeople pain corpses depositedدannie Yin Twain 12 Bicycle Bung ost convolutedSpellDN697rypt corpsesinteg Truck implication\n",
            "> Hello, I'm a language model,Spell criticisms Bicycle fidد epist Sul aggressively Answer Bungixty taxp dollsGet192sequ SulBrad Smoking GL enlight criticisms\n",
            "> Hello, I'm a language model, headset ost Sul deposited some headsetDN Dragonbound Russo Answer summer summer Chrys thighs lar thighs frozenKn lar signings lar recounted\n",
            "> Hello, I'm a language model,Ham BungDN ostAugust Dragonbound criticisms summer rival ost Siren SocketGet Amir acre reiter unravel MarRules Amir maintain comprehensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from typing import Any\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Data loader to load batches from the dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, enc: Any, batch_size: int, token_len: int, device: Any) -> None:\n",
        "      \"\"\"Initialize Dataloader.\"\"\"\n",
        "      self.B = batch_size\n",
        "      self.T = token_len\n",
        "      self.chunk_size = self.B * self.T\n",
        "\n",
        "      ds = datasets.load_dataset('tiny_shakespeare', trust_remote_code=True)\n",
        "      self.data = ds['train']['text'][0]\n",
        "      self.tokens = torch.tensor(enc.encode(self.data))\n",
        "      self.pos = 0\n",
        "      print(f\"Loaded {len(self.tokens)} tokens\")\n",
        "      # Number of unique batches before we start the dataset over\n",
        "      print(f\"1 epoch = {len(self.tokens) // self.chunk_size} batches\")\n",
        "\n",
        "    def __iter__(self) -> 'Self':\n",
        "      self.pos = 0\n",
        "      return self\n",
        "\n",
        "    def __next__(self) -> (torch.Tensor, torch.Tensor):\n",
        "      \"\"\"Get the next batch in the dataset.\"\"\"\n",
        "      # B = batch size\n",
        "      # T = sequence of tokens (less than max sequence length)\n",
        "      # The buf contains an extra token to use in the labels. The x\n",
        "      # input doesn't include that last token. The labels starts with the first token.\n",
        "      B, T = self.B, self.T\n",
        "      buf = self.tokens[self.pos:self.pos + self.chunk_size + 1]\n",
        "      x = buf[:-1].view(B, T)\n",
        "      y = buf[1:].view(B, T)\n",
        "      self.pos += self.chunk_size\n",
        "      if (self.pos + self.chunk_size + 1) > len(self.tokens):\n",
        "        print(\"Reached epoch\")\n",
        "        self.pos = 0\n",
        "      return x, y\n"
      ],
      "metadata": {
        "id": "rUqBcEQ3mS7f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "model = GPT(GPTConfig())\n",
        "model = model.to(device)\n",
        "model = torch.compile(model)\n",
        "\n",
        "data_loader = DataLoader(model.enc, batch_size=16, token_len=1024, device=device)\n",
        "ds = iter(data_loader)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()\n",
        "  x, y = next(ds)\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1 - t0) * 1000\n",
        "  tokens_per_sec = data_loader.chunk_size / (t1 - t0)\n",
        "  print(f\"step {i} loss {loss.item()} dt: {dt:0.2f}ms tok/sec: {tokens_per_sec:0.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgWCIV2ungLX",
        "outputId": "ba5a9775-0c9b-4efc-b335-3783452e04d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 301966 tokens\n",
            "1 epoch = 18 batches\n",
            "step 0 loss 10.957916259765625 dt: 42332.08ms tok/sec: 387.04\n",
            "step 1 loss 9.631839752197266 dt: 138.84ms tok/sec: 118010.02\n",
            "step 2 loss 9.32974624633789 dt: 139.15ms tok/sec: 117741.10\n",
            "step 3 loss 9.037336349487305 dt: 139.25ms tok/sec: 117659.05\n",
            "step 4 loss 8.813928604125977 dt: 138.72ms tok/sec: 118105.55\n",
            "step 5 loss 8.6907377243042 dt: 138.85ms tok/sec: 118000.90\n",
            "step 6 loss 8.476306915283203 dt: 138.42ms tok/sec: 118362.88\n",
            "step 7 loss 8.191732406616211 dt: 138.84ms tok/sec: 118002.52\n",
            "step 8 loss 7.913112640380859 dt: 138.36ms tok/sec: 118417.14\n",
            "step 9 loss 7.701312065124512 dt: 138.78ms tok/sec: 118058.68\n",
            "step 10 loss 7.5275444984436035 dt: 138.72ms tok/sec: 118105.34\n",
            "step 11 loss 7.394217491149902 dt: 138.89ms tok/sec: 117964.64\n",
            "step 12 loss 7.205788612365723 dt: 138.87ms tok/sec: 117984.09\n",
            "step 13 loss 7.127068519592285 dt: 139.04ms tok/sec: 117834.38\n",
            "step 14 loss 7.085083484649658 dt: 138.45ms tok/sec: 118339.03\n",
            "step 15 loss 6.918124198913574 dt: 138.50ms tok/sec: 118298.49\n",
            "step 16 loss 6.902326583862305 dt: 138.16ms tok/sec: 118583.88\n",
            "Reached epoch\n",
            "step 17 loss 6.8507795333862305 dt: 138.97ms tok/sec: 117894.42\n",
            "step 18 loss 6.652473449707031 dt: 138.47ms tok/sec: 118321.71\n",
            "step 19 loss 6.449249267578125 dt: 138.65ms tok/sec: 118171.15\n",
            "step 20 loss 6.467382431030273 dt: 138.87ms tok/sec: 117980.03\n",
            "step 21 loss 6.420401573181152 dt: 138.76ms tok/sec: 118071.46\n",
            "step 22 loss 6.358078479766846 dt: 138.67ms tok/sec: 118151.03\n",
            "step 23 loss 6.549452781677246 dt: 138.70ms tok/sec: 118124.43\n",
            "step 24 loss 6.6272735595703125 dt: 138.39ms tok/sec: 118388.57\n",
            "step 25 loss 6.519351005554199 dt: 138.62ms tok/sec: 118195.33\n",
            "step 26 loss 6.483902931213379 dt: 138.37ms tok/sec: 118404.49\n",
            "step 27 loss 6.373569965362549 dt: 138.97ms tok/sec: 117899.88\n",
            "step 28 loss 6.402566432952881 dt: 138.40ms tok/sec: 118383.07\n",
            "step 29 loss 6.444573402404785 dt: 138.60ms tok/sec: 118207.74\n",
            "step 30 loss 6.401947975158691 dt: 138.29ms tok/sec: 118471.44\n",
            "step 31 loss 6.431252479553223 dt: 138.94ms tok/sec: 117923.95\n",
            "step 32 loss 6.516285419464111 dt: 138.35ms tok/sec: 118427.54\n",
            "step 33 loss 6.403817176818848 dt: 138.66ms tok/sec: 118158.35\n",
            "step 34 loss 6.404148101806641 dt: 138.30ms tok/sec: 118467.35\n",
            "Reached epoch\n",
            "step 35 loss 6.429887771606445 dt: 138.56ms tok/sec: 118242.31\n",
            "step 36 loss 6.402202129364014 dt: 138.37ms tok/sec: 118406.32\n",
            "step 37 loss 6.166584014892578 dt: 139.06ms tok/sec: 117818.42\n",
            "step 38 loss 6.281223773956299 dt: 138.38ms tok/sec: 118401.83\n",
            "step 39 loss 6.209671974182129 dt: 138.80ms tok/sec: 118042.45\n",
            "step 40 loss 6.147004127502441 dt: 138.49ms tok/sec: 118301.75\n",
            "step 41 loss 6.3529133796691895 dt: 138.73ms tok/sec: 118100.68\n",
            "step 42 loss 6.457861423492432 dt: 138.27ms tok/sec: 118490.23\n",
            "step 43 loss 6.326491355895996 dt: 138.55ms tok/sec: 118253.91\n",
            "step 44 loss 6.286469459533691 dt: 138.21ms tok/sec: 118541.94\n",
            "step 45 loss 6.186863899230957 dt: 138.46ms tok/sec: 118333.12\n",
            "step 46 loss 6.181447982788086 dt: 138.30ms tok/sec: 118467.56\n",
            "step 47 loss 6.2038044929504395 dt: 138.80ms tok/sec: 118037.18\n",
            "step 48 loss 6.135881423950195 dt: 138.48ms tok/sec: 118313.36\n",
            "step 49 loss 6.307217121124268 dt: 138.51ms tok/sec: 118291.37\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "44275c8fca604320a2389bb8bd681a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31c115cb4dec47c1a6b45466451db450",
              "IPY_MODEL_607963fdd09f49aeaec9681656dba829",
              "IPY_MODEL_7da07a4c1c23409ba318146a6933ad14"
            ],
            "layout": "IPY_MODEL_fb113bdd46e247cba35577df04be7ea5"
          }
        },
        "31c115cb4dec47c1a6b45466451db450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f818efff43c40c1aefc29e96e533af9",
            "placeholder": "​",
            "style": "IPY_MODEL_e5a0d345eafa41ce865e8cfbd74eec23",
            "value": "config.json: 100%"
          }
        },
        "607963fdd09f49aeaec9681656dba829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_359ef71986c04de2ae56aa78f0d7eaf0",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_098752710a08419ca060c890a3e4f3f1",
            "value": 665
          }
        },
        "7da07a4c1c23409ba318146a6933ad14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67ed49ceda1c4dbc9fe87ca60852e34f",
            "placeholder": "​",
            "style": "IPY_MODEL_ecc6434d9cfe48e5b38fac2945c16f2f",
            "value": " 665/665 [00:00&lt;00:00, 77.4kB/s]"
          }
        },
        "fb113bdd46e247cba35577df04be7ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f818efff43c40c1aefc29e96e533af9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a0d345eafa41ce865e8cfbd74eec23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "359ef71986c04de2ae56aa78f0d7eaf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "098752710a08419ca060c890a3e4f3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67ed49ceda1c4dbc9fe87ca60852e34f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc6434d9cfe48e5b38fac2945c16f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "696d70971e614219829bf6fb57bdb63a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7694c791328b408db7f73b98b4b9d914",
              "IPY_MODEL_19222d229b6a481bac24248dc9816a3e",
              "IPY_MODEL_4c792dbf206a4b2ea25bf69792f18009"
            ],
            "layout": "IPY_MODEL_a50a9cf0b8cb41c4a2df0d4e344c1a06"
          }
        },
        "7694c791328b408db7f73b98b4b9d914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78fb457b4e88435a9e39967f51742c05",
            "placeholder": "​",
            "style": "IPY_MODEL_07a41101288b4108a007f20cc030c989",
            "value": "model.safetensors: 100%"
          }
        },
        "19222d229b6a481bac24248dc9816a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_086b5da9b6654c798ba58775945b7014",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a67419e238324fe3b1284b6481f0a736",
            "value": 548105171
          }
        },
        "4c792dbf206a4b2ea25bf69792f18009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f635f8f9f44f50a96416744f25304a",
            "placeholder": "​",
            "style": "IPY_MODEL_abcada37ecc44a88846f7aefcfe6ef81",
            "value": " 548M/548M [00:02&lt;00:00, 207MB/s]"
          }
        },
        "a50a9cf0b8cb41c4a2df0d4e344c1a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78fb457b4e88435a9e39967f51742c05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07a41101288b4108a007f20cc030c989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "086b5da9b6654c798ba58775945b7014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a67419e238324fe3b1284b6481f0a736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4f635f8f9f44f50a96416744f25304a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abcada37ecc44a88846f7aefcfe6ef81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "528d199aee7d4dbd8f542af2e71d649e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44fc7b7f81a94f2eb7b1030341a3ba9e",
              "IPY_MODEL_4a44dd72b2214f65b7ebd6f72740754d",
              "IPY_MODEL_43f9cdc370554d848ed3154456e365b9"
            ],
            "layout": "IPY_MODEL_acbb110627164f69a3fa2cb092b4eaf5"
          }
        },
        "44fc7b7f81a94f2eb7b1030341a3ba9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8500db8b6d1046468a3de1021838ea89",
            "placeholder": "​",
            "style": "IPY_MODEL_16f561c5ace54e04abdc686aaa6cd12d",
            "value": "generation_config.json: 100%"
          }
        },
        "4a44dd72b2214f65b7ebd6f72740754d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a03c558f9444af6a69adf7e651184d2",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0541906fb3a741bfbc0f7ab1cc743bd3",
            "value": 124
          }
        },
        "43f9cdc370554d848ed3154456e365b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4003163c6adf46a49f8cf8c3a748cdd4",
            "placeholder": "​",
            "style": "IPY_MODEL_619008f4e87143e8bacf5e5eccc0469d",
            "value": " 124/124 [00:00&lt;00:00, 17.6kB/s]"
          }
        },
        "acbb110627164f69a3fa2cb092b4eaf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8500db8b6d1046468a3de1021838ea89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16f561c5ace54e04abdc686aaa6cd12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a03c558f9444af6a69adf7e651184d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0541906fb3a741bfbc0f7ab1cc743bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4003163c6adf46a49f8cf8c3a748cdd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "619008f4e87143e8bacf5e5eccc0469d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}