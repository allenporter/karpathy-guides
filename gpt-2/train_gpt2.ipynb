{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3hwv5SmU0B2",
        "outputId": "459143b2-351c-42df-93e7-f2ac972ffede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, tiktoken, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 tiktoken-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dzpvIRu4QKzg",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import math\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    \"\"\"This class defines the configuration for the GPT model.\"\"\"\n",
        "\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Attention module.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        \"\"\"Initialize MLP.\"\"\"\n",
        "        super().__init__()\n",
        "        # Batch of key/query/value projects for all heads\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.SCALE_INIT = 1\n",
        "        # Regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed = config.n_embd\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
        "                1, 1, config.block_size, config.block_size\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Perform inference.\"\"\"\n",
        "        B, T, C = x.size()\n",
        "        # Compute the query, key, value for all heads in the batch.\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
        "        # Each are (B, nh, T, hs)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        # # attention materializes (T, T)\n",
        "        # # Queries and keys interact\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # # Ensure tokens only attend to tokens before them and not to tokens in the future\n",
        "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        # # Normalize attention\n",
        "        # att = F.softmax(att, dim=-1)\n",
        "        # # Compute a weighted sum of interesting tokens\n",
        "        # y = att @ v\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        # Reassemble and concat everything\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        # Output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"Multi-layer perceptron.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        \"\"\"Initialize MLP.\"\"\"\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Perform inference.\"\"\"\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"A transformer block.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        \"\"\"Initialize Block.\"\"\"\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Perform inference.\"\"\"\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLsVuKoNQKzh",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"This class defines the GPT model.\"\"\"\n",
        "\n",
        "    def __init__(self, config: GPTConfig) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            {\n",
        "                \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                \"wpe\": nn.Embedding(config.block_size, config.n_embd),\n",
        "                \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                \"ln_f\": nn.LayerNorm(config.n_embd),\n",
        "            }\n",
        "        )\n",
        "        # Final classifier\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "        # Share weights for input and output embeddings. This is about 30% of\n",
        "        # the model weights.\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def _init_weights(self, module: nn.Module) -> None:\n",
        "        \"\"\"Perform additional weight initialization to match gpt-2.\"\"\"\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "            if hasattr(module, \"SCALE_INIT\"):\n",
        "            std *= (2 * self.config.n_layer) ** -0.05\n",
        "            torch.nn.init.normal_(module.weight, mean=0, std=std)\n",
        "            if module.bias is not None:\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0, std=std)\n",
        "\n",
        "    def configure_optimizers(self, weight_decay: float, learning_rate: float, device: Any ) -> torch.optim.AdamW\n",
        "        \"\"\"Return the optimizer.\"\"\"\n",
        "        # start with all params that require grad\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters() }\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim optim_groups\n",
        "        decay_params = {p for p in param_dict.valus() if p.dim() >= 2 }\n",
        "        nodecay_params = {p for p in param_dict.valus() if p.dim() < 2 }\n",
        "        optim_groups = {\n",
        "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "            {\"params\": nodecay_params, \"weight_decay\": 0},\n",
        "        }\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decay_params {len(decay_params)}\")\n",
        "        print(f\"num nodecay_params {len(nodecay_params)}\")\n",
        "        fused_available = \"fused\" in inspect.signature(torch.optim.Adamw).parameters\n",
        "        use_fused = fused_available and \"cuda\" in device\n",
        "        print(\"Using fused adamw: {use_fused}\")\n",
        "        return torch.optim.AdamW(optim_groups, lr=3e-4, betas=(0.9, 0.95), eps=1e-8, use_fused=use_fused)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                targets: torch.Tensor | None = None\n",
        "      ) -> (torch.Tensor, float):\n",
        "        \"\"\"Perform generation.\"\"\"\n",
        "        B, T = x.size()\n",
        "        assert T <= self.config.block_size  # Max sequence length\n",
        "        # Forward token and positional embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=x.device)  # Shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos)  # (T, n_emb)\n",
        "        tok_emb = self.transformer.wte(x)  # (B, T, n_emb)\n",
        "        x = tok_emb + pos_emb\n",
        "        # Forward transformer blocks\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # Forward the final layernorm\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "          loss = F.cross_entropy(\n",
        "              # Flatten to (BxT, vocab_size)\n",
        "              logits.view(-1, logits.size(-1)),\n",
        "              # Flatten to (BxT)\n",
        "              targets.view(-1)\n",
        "          )\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type: str) -> \"GPT\":\n",
        "        \"\"\"Load the GPT from the pretrained model.\"\"\"\n",
        "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n",
        "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
        "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n",
        "        }[model_type]\n",
        "        config_args[\"vocab_size\"] = 50257  # always 50257 for GPT model checkpoints\n",
        "        config_args[\"block_size\"] = 1024  # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [\n",
        "            k for k in sd_keys if not k.endswith(\".attn.bias\")\n",
        "        ]  # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")\n",
        "        ]  # ignore these, just a buffer\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.bias\")\n",
        "        ]  # same, just the mask (buffer)\n",
        "        transposed = [\n",
        "            \"attn.c_attn.weight\",\n",
        "            \"attn.c_proj.weight\",\n",
        "            \"mlp.c_fc.weight\",\n",
        "            \"mlp.c_proj.weight\",\n",
        "        ]\n",
        "\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(\n",
        "            sd_keys\n",
        "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def sample(self, text: str, num_return_sequences: int, max_length: int) -> list[str]:\n",
        "      \"\"\"Sample from the model from text input.\"\"\"\n",
        "      tokens = self.enc.encode(text)\n",
        "      tokens = torch.tensor(tokens, dtype=torch.long) # (8, )\n",
        "      # Replicate input tokens\n",
        "      tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "\n",
        "      # x is (B, T)\n",
        "      x = tokens.to(device)\n",
        "\n",
        "      # With each loop iteration we'll append a token to the sequence. This is\n",
        "      # adding one more column to x each time.\n",
        "      while x.size(1) < max_length:\n",
        "        with torch.no_grad():\n",
        "          logits, _ = model(x)  # (B, T, vocab_size)\n",
        "          # Take the logits at the last position (next character) and drop the others.\n",
        "          # This is correct but inefficient implementation of sampling.\n",
        "          # Question: What is T?\n",
        "          logits = logits[:, -1, :]  # (B, vocab_size)\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          # Do top-k sampling of 50 which is the huggingface default. Get the top 50\n",
        "          # probabilities and set all other tokens to probability of zero. This helps\n",
        "          # keep the model on track so it doesn't go off the rails as easily.\n",
        "          # Both are (5, 50)\n",
        "          topk_probs, topk_indicies = torch.topk(probs, 50, dim=-1)\n",
        "          # Select a token from the top 5\n",
        "          ix = torch.multinomial(topk_probs, 1)  # (B, 1)\n",
        "          # Gather corresponding indicidies\n",
        "          xcol = torch.gather(topk_indicies, -1, ix)\n",
        "          # Append the new character to the sequence (one for each in the batch)\n",
        "          x = torch.cat((x, xcol), dim=-1)\n",
        "\n",
        "      samples = []\n",
        "      for i in range(num_return_sequences):\n",
        "        tokens = x[i, :max_length].tolist()\n",
        "        decoded = self.enc.decode(tokens)\n",
        "        samples.append(decoded)\n",
        "\n",
        "      return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybm3ZqUUkFvP"
      },
      "source": [
        "# From Pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "dfc2264726fd4445a2854b9de7ff2159",
            "ed4ddcabd2304a12a84d5dd261d0b63d",
            "765f57b7f5034bf6be765f05572b6ab2",
            "6bbd93c025ed47378aae2ea6bcf540de",
            "a32b1ca158914eeabbc811e73b4250cd",
            "4aba223adb904dc3966b41d19770d950",
            "96bcbd074aaf46e28f6ec1d95abda49b",
            "9e3124b736cc4faaa8a81cb589504308",
            "c5037aafea364537b0e246527c520387",
            "6275c8aaaf8b4eeaabb7c9c99ab106f6",
            "332c6d02a92d4ff383355c144453c583",
            "58349f2da85b4ed69cf1844f2d795e20",
            "d2b45d7cd65242cc9a178dfe53f570a6",
            "43739353d89a44fdbd65d1eca2fbbe61",
            "1655b2cc6f5b48e1b9e4ce554c8ffdca",
            "c00973e47ef04ad4895aa8b235646432",
            "893cd37121564cfdb836ca561bb95b9d",
            "225d9b22b80741d2bea458092172aeee",
            "567c3a648b71496eb727f8ff4e7d2d0b",
            "f0c3368183a0434fb78498e00f4d1b63",
            "ca8dac75f81b4e42aed1ce4bc5885a98",
            "4401c4b1ae0c46be95a0e1a0cf3d7559",
            "c3e521a5ecda4e049d5baaea6ce828fc",
            "af087ea4c3774aa9ab66c6ff8dc6d35c",
            "e1080a455ef648d2b81ef69b873cf640",
            "28fc34cc01ff4a23873c5e1cb42ef16d",
            "cd0031deab4649298c5d14309790402e",
            "fd14415e4a13495b94ff4bbefe712c70",
            "cafa26a4c8004a6d971d8f430618e161",
            "4f8495417cec4a9cbb71dd5c10743e92",
            "5f543b00beda48faab42f13871b15681",
            "d3b6b012f8b14fa98ab46931650d4a31",
            "7c4e3b6466674c9da96346938f2ef629"
          ]
        },
        "id": "EQQrlzi6QKzh",
        "outputId": "3d5eacc5-556f-479b-b92a-2260928eca55",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfc2264726fd4445a2854b9de7ff2159",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58349f2da85b4ed69cf1844f2d795e20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3e521a5ecda4e049d5baaea6ce828fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = GPT.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ia9BolweAx2",
        "outputId": "b8d9b265-0fe2-46f2-eb38-9879cc101c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Hello, I'm a language model, not a program.\n",
            "\n",
            "So this morning I started studying for the interview in the lab. This was not\n",
            "> Hello, I'm a language model, and one of the main things that bothers me when they create languages is how easy it becomes to create something that\n",
            "> Hello, I'm a language model, and I wrote it off on the grounds that a language model would make me more fluent. But I'm not\n",
            "> Hello, I'm a language model, I really like languages. I like languages because like, they're good. And the way we talk about languages\n",
            "> Hello, I'm a language model, a language model I'm using for data modelling. All I did was test the results and then I wrote some\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "samples = model.sample(\"Hello, I'm a language model,\", num_return_sequences, max_length)\n",
        "for sample in samples:\n",
        "  print(\">\", sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfaK80mbkDXa"
      },
      "source": [
        "# Train from Random Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qxqeIdmTkKYE"
      },
      "outputs": [],
      "source": [
        "model = GPT(GPTConfig())\n",
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVo4NuQjlth-",
        "outputId": "d3f44fa2-0793-4ed3-a4b5-56127f6e50f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Hello, I'm a language model, Twice697 rival Furthermore Furthermore EditorsDN forfe smilesmithDN shelter 12神DN stabbed 700 behaving Warning thighs analges analges\n",
            "> Hello, I'm a language model, regardpeople pain corpses depositedدannie Yin Twain 12 Bicycle Bung ost convolutedSpellDN697rypt corpsesinteg Truck implication\n",
            "> Hello, I'm a language model,Spell criticisms Bicycle fidد epist Sul aggressively Answer Bungixty taxp dollsGet192sequ SulBrad Smoking GL enlight criticisms\n",
            "> Hello, I'm a language model, headset ost Sul deposited some headsetDN Dragonbound Russo Answer summer summer Chrys thighs lar thighs frozenKn lar signings lar recounted\n",
            "> Hello, I'm a language model,Ham BungDN ostAugust Dragonbound criticisms summer rival ost Siren SocketGet Amir acre reiter unravel MarRules Amir maintain comprehensive\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "samples = model.sample(\"Hello, I'm a language model,\", num_return_sequences, max_length)\n",
        "for sample in samples:\n",
        "  print(\">\", sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rUqBcEQ3mS7f"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from typing import Any\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Data loader to load batches from the dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, enc: Any, batch_size: int, token_len: int, device: Any) -> None:\n",
        "      \"\"\"Initialize Dataloader.\"\"\"\n",
        "      self.B = batch_size\n",
        "      self.T = token_len\n",
        "      self.chunk_size = self.B * self.T\n",
        "\n",
        "      ds = datasets.load_dataset('tiny_shakespeare', trust_remote_code=True)\n",
        "      self.data = ds['train']['text'][0]\n",
        "      self.tokens = torch.tensor(enc.encode(self.data))\n",
        "      self.pos = 0\n",
        "      print(f\"Loaded {len(self.tokens)} tokens\")\n",
        "      # Number of unique batches before we start the dataset over\n",
        "      print(f\"1 epoch = {len(self.tokens) // self.chunk_size} batches\")\n",
        "\n",
        "    def __iter__(self) -> 'Self':\n",
        "      self.pos = 0\n",
        "      return self\n",
        "\n",
        "    def __next__(self) -> (torch.Tensor, torch.Tensor):\n",
        "      \"\"\"Get the next batch in the dataset.\"\"\"\n",
        "      # B = batch size\n",
        "      # T = sequence of tokens (less than max sequence length)\n",
        "      # The buf contains an extra token to use in the labels. The x\n",
        "      # input doesn't include that last token. The labels starts with the first token.\n",
        "      B, T = self.B, self.T\n",
        "      buf = self.tokens[self.pos:self.pos + self.chunk_size + 1]\n",
        "      x = buf[:-1].view(B, T)\n",
        "      y = buf[1:].view(B, T)\n",
        "      self.pos += self.chunk_size\n",
        "      if (self.pos + self.chunk_size + 1) > len(self.tokens):\n",
        "        print(\"Reached epoch\")\n",
        "        self.pos = 0\n",
        "      return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgWCIV2ungLX",
        "outputId": "9a81fce5-a93f-4074-b3fe-3778473080a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 301966 tokens\n",
            "1 epoch = 18 batches\n",
            "step 0 loss 10.98756217956543 dt: 24409.04ms tok/sec: 671.23\n",
            "step 1 loss 9.523605346679688 dt: 93.32ms tok/sec: 175559.62\n",
            "step 2 loss 9.380006790161133 dt: 92.60ms tok/sec: 176928.29\n",
            "step 3 loss 9.06810474395752 dt: 92.39ms tok/sec: 177334.64\n",
            "step 4 loss 8.87963581085205 dt: 92.64ms tok/sec: 176850.42\n",
            "step 5 loss 8.785743713378906 dt: 92.51ms tok/sec: 177108.40\n",
            "step 6 loss 8.55899429321289 dt: 92.40ms tok/sec: 177309.01\n",
            "step 7 loss 8.271232604980469 dt: 92.71ms tok/sec: 176728.08\n",
            "step 8 loss 7.940434455871582 dt: 92.45ms tok/sec: 177215.73\n",
            "step 9 loss 7.704261779785156 dt: 93.04ms tok/sec: 176093.16\n",
            "step 10 loss 7.534992218017578 dt: 93.16ms tok/sec: 175875.49\n",
            "step 11 loss 7.438817977905273 dt: 92.82ms tok/sec: 176510.19\n",
            "step 12 loss 7.250419616699219 dt: 92.54ms tok/sec: 177044.98\n",
            "step 13 loss 7.1727294921875 dt: 92.77ms tok/sec: 176606.81\n",
            "step 14 loss 7.112596035003662 dt: 92.48ms tok/sec: 177162.28\n",
            "step 15 loss 6.942439556121826 dt: 92.52ms tok/sec: 177081.47\n",
            "step 16 loss 6.909427642822266 dt: 93.00ms tok/sec: 176164.04\n",
            "Reached epoch\n",
            "step 17 loss 6.864141464233398 dt: 92.89ms tok/sec: 176382.43\n",
            "step 18 loss 6.683676719665527 dt: 92.63ms tok/sec: 176875.00\n",
            "step 19 loss 6.481067180633545 dt: 92.97ms tok/sec: 176230.90\n",
            "step 20 loss 6.503272533416748 dt: 94.70ms tok/sec: 173014.99\n",
            "step 21 loss 6.44672155380249 dt: 93.96ms tok/sec: 174363.14\n",
            "step 22 loss 6.36313009262085 dt: 92.78ms tok/sec: 176598.19\n",
            "step 23 loss 6.554483413696289 dt: 92.42ms tok/sec: 177275.16\n",
            "step 24 loss 6.623485565185547 dt: 92.50ms tok/sec: 177128.94\n",
            "step 25 loss 6.527750015258789 dt: 92.70ms tok/sec: 176734.44\n",
            "step 26 loss 6.480316162109375 dt: 92.61ms tok/sec: 176912.34\n",
            "step 27 loss 6.376148700714111 dt: 92.52ms tok/sec: 177084.67\n",
            "step 28 loss 6.403698921203613 dt: 93.16ms tok/sec: 175875.94\n",
            "step 29 loss 6.446211814880371 dt: 92.65ms tok/sec: 176844.51\n",
            "step 30 loss 6.4005889892578125 dt: 92.65ms tok/sec: 176840.87\n",
            "step 31 loss 6.443240165710449 dt: 92.69ms tok/sec: 176768.54\n",
            "step 32 loss 6.5359721183776855 dt: 92.48ms tok/sec: 177156.80\n",
            "step 33 loss 6.42534065246582 dt: 92.53ms tok/sec: 177058.21\n",
            "step 34 loss 6.444929599761963 dt: 92.53ms tok/sec: 177074.17\n",
            "Reached epoch\n",
            "step 35 loss 6.47804069519043 dt: 92.43ms tok/sec: 177259.62\n",
            "step 36 loss 6.445855140686035 dt: 92.40ms tok/sec: 177325.48\n",
            "step 37 loss 6.222194671630859 dt: 92.58ms tok/sec: 176973.39\n",
            "step 38 loss 6.3283610343933105 dt: 92.38ms tok/sec: 177347.91\n",
            "step 39 loss 6.280583381652832 dt: 92.44ms tok/sec: 177248.64\n",
            "step 40 loss 6.2301025390625 dt: 92.59ms tok/sec: 176956.99\n",
            "step 41 loss 6.442305564880371 dt: 92.44ms tok/sec: 177244.99\n",
            "step 42 loss 6.544681549072266 dt: 92.36ms tok/sec: 177401.02\n",
            "step 43 loss 6.417820930480957 dt: 92.65ms tok/sec: 176834.04\n",
            "step 44 loss 6.384302616119385 dt: 92.48ms tok/sec: 177165.02\n",
            "step 45 loss 6.276423454284668 dt: 92.41ms tok/sec: 177293.46\n",
            "step 46 loss 6.299271583557129 dt: 92.56ms tok/sec: 177015.79\n",
            "step 47 loss 6.325965404510498 dt: 92.32ms tok/sec: 177464.24\n",
            "step 48 loss 6.2513885498046875 dt: 92.39ms tok/sec: 177327.31\n",
            "step 49 loss 6.322411060333252 dt: 92.65ms tok/sec: 176837.23\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "model = model.to(device)\n",
        "model = torch.compile(model)\n",
        "\n",
        "data_loader = DataLoader(model.enc, batch_size=16, token_len=1024, device=device)\n",
        "ds = iter(data_loader)\n",
        "\n",
        "\n",
        "class TrainState:\n",
        "  \"\"\"GPT-3 learning rate.\"\"\"\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    \"\"\"Init Learning rate.\"\"\"\n",
        "    self.max_lr = 6e-4\n",
        "    self.min_lr = self.max_lr * 0.1\n",
        "    self.warmup_steps = 10\n",
        "    self.max_steps = 50\n",
        "\n",
        "  def get_lr(self, step: int) -> float:\n",
        "    \"\"\"Learning rate.\"\"\"\n",
        "    if step < self.warmup_steps:\n",
        "      return self.max_lr * (step + 1) / self.warmup_steps\n",
        "    if step > self.max_steps:\n",
        "      return self.min_lr\n",
        "    decay_ratio = (step - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return self.min_lr + coeff * (self.max_lr - self.min_lr)\n",
        "\n",
        "state = TrainState()\n",
        "\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, lr=3e-4, device=device)\n",
        "\n",
        "for step in range(state.max_steps):\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()\n",
        "  x, y = next(ds)\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  # Prevent the model from getting large shocks of gradient magnitude\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  # Determine the learning rate\n",
        "  lr = state.get_lr(step)\n",
        "  for param_group in optimizer.param_group:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1 - t0) * 1000\n",
        "  tokens_per_sec = data_loader.chunk_size / (t1 - t0)\n",
        "  print(f\"step {i} loss {loss.item():0.6f} norm: {norm:0.4f} dt: {dt:0.2f}ms tok/sec: {tokens_per_sec:0.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1655b2cc6f5b48e1b9e4ce554c8ffdca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8dac75f81b4e42aed1ce4bc5885a98",
            "placeholder": "​",
            "style": "IPY_MODEL_4401c4b1ae0c46be95a0e1a0cf3d7559",
            "value": " 548M/548M [00:02&lt;00:00, 231MB/s]"
          }
        },
        "225d9b22b80741d2bea458092172aeee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28fc34cc01ff4a23873c5e1cb42ef16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3b6b012f8b14fa98ab46931650d4a31",
            "placeholder": "​",
            "style": "IPY_MODEL_7c4e3b6466674c9da96346938f2ef629",
            "value": " 124/124 [00:00&lt;00:00, 16.5kB/s]"
          }
        },
        "332c6d02a92d4ff383355c144453c583": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43739353d89a44fdbd65d1eca2fbbe61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_567c3a648b71496eb727f8ff4e7d2d0b",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0c3368183a0434fb78498e00f4d1b63",
            "value": 548105171
          }
        },
        "4401c4b1ae0c46be95a0e1a0cf3d7559": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aba223adb904dc3966b41d19770d950": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8495417cec4a9cbb71dd5c10743e92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "567c3a648b71496eb727f8ff4e7d2d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58349f2da85b4ed69cf1844f2d795e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2b45d7cd65242cc9a178dfe53f570a6",
              "IPY_MODEL_43739353d89a44fdbd65d1eca2fbbe61",
              "IPY_MODEL_1655b2cc6f5b48e1b9e4ce554c8ffdca"
            ],
            "layout": "IPY_MODEL_c00973e47ef04ad4895aa8b235646432"
          }
        },
        "5f543b00beda48faab42f13871b15681": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6275c8aaaf8b4eeaabb7c9c99ab106f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bbd93c025ed47378aae2ea6bcf540de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6275c8aaaf8b4eeaabb7c9c99ab106f6",
            "placeholder": "​",
            "style": "IPY_MODEL_332c6d02a92d4ff383355c144453c583",
            "value": " 665/665 [00:00&lt;00:00, 82.7kB/s]"
          }
        },
        "765f57b7f5034bf6be765f05572b6ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e3124b736cc4faaa8a81cb589504308",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5037aafea364537b0e246527c520387",
            "value": 665
          }
        },
        "7c4e3b6466674c9da96346938f2ef629": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "893cd37121564cfdb836ca561bb95b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96bcbd074aaf46e28f6ec1d95abda49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e3124b736cc4faaa8a81cb589504308": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32b1ca158914eeabbc811e73b4250cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af087ea4c3774aa9ab66c6ff8dc6d35c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd14415e4a13495b94ff4bbefe712c70",
            "placeholder": "​",
            "style": "IPY_MODEL_cafa26a4c8004a6d971d8f430618e161",
            "value": "generation_config.json: 100%"
          }
        },
        "c00973e47ef04ad4895aa8b235646432": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3e521a5ecda4e049d5baaea6ce828fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af087ea4c3774aa9ab66c6ff8dc6d35c",
              "IPY_MODEL_e1080a455ef648d2b81ef69b873cf640",
              "IPY_MODEL_28fc34cc01ff4a23873c5e1cb42ef16d"
            ],
            "layout": "IPY_MODEL_cd0031deab4649298c5d14309790402e"
          }
        },
        "c5037aafea364537b0e246527c520387": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca8dac75f81b4e42aed1ce4bc5885a98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cafa26a4c8004a6d971d8f430618e161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd0031deab4649298c5d14309790402e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2b45d7cd65242cc9a178dfe53f570a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_893cd37121564cfdb836ca561bb95b9d",
            "placeholder": "​",
            "style": "IPY_MODEL_225d9b22b80741d2bea458092172aeee",
            "value": "model.safetensors: 100%"
          }
        },
        "d3b6b012f8b14fa98ab46931650d4a31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfc2264726fd4445a2854b9de7ff2159": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed4ddcabd2304a12a84d5dd261d0b63d",
              "IPY_MODEL_765f57b7f5034bf6be765f05572b6ab2",
              "IPY_MODEL_6bbd93c025ed47378aae2ea6bcf540de"
            ],
            "layout": "IPY_MODEL_a32b1ca158914eeabbc811e73b4250cd"
          }
        },
        "e1080a455ef648d2b81ef69b873cf640": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f8495417cec4a9cbb71dd5c10743e92",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f543b00beda48faab42f13871b15681",
            "value": 124
          }
        },
        "ed4ddcabd2304a12a84d5dd261d0b63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aba223adb904dc3966b41d19770d950",
            "placeholder": "​",
            "style": "IPY_MODEL_96bcbd074aaf46e28f6ec1d95abda49b",
            "value": "config.json: 100%"
          }
        },
        "f0c3368183a0434fb78498e00f4d1b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd14415e4a13495b94ff4bbefe712c70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
