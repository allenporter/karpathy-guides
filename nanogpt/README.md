# nanogpt

This is following along in the Youtube Video [Let's build GPT: from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=588s)

This is effectively implementing a decoder only transformer without cross attention.